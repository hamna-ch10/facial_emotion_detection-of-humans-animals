# -*- coding: utf-8 -*-
"""facialemotion_detection_28_nov_24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcj2vzgy6DyQvGUG1p36PyCi6p0uNfZ3
"""

import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds
from datetime import datetime
from packaging import version

import numpy as np
import matplotlib.pyplot as plt
import glob
import cv2
import tensorflow as tf
from keras.models import Model, Sequential
from keras import layers
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D
import os
import seaborn as sns
from tensorflow.keras import losses, Model
import keras
import numpy as np
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

!unrar x -y /content/drive/MyDrive/DATASETS.rar -d /content/drive/MyDrive

SIZE = 256  #Resize images

"""## Resizing data
## Just Run for once

"""

##
#Capture training data and labels into respective lists
images_resize = []
labels_resize = []
from PIL import Image
path = "/content/drive/MyDrive"
save_directory = 'datasetnew'
for directory_path in glob.glob(path+"/DATASETS/*"):
    label = directory_path.split("/")[-1]
    save = os.path.join(path,save_directory, label)
    print(save)
    counter = 0
    if False == os.path.isdir(save):
      os.mkdir(save)
    for img_path in glob.glob(os.path.join(directory_path, "*")):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)
        img = cv2.resize(img, (SIZE, SIZE))
        # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        counter+=1
        cv2.imwrite(os.path.join(save, str(counter)+'.png'), img)

        images_resize.append(img)
        labels_resize.append(label)

print(len(images_resize))

#Capture training data and labels into respective lists
images = []
labels = []

for directory_path in glob.glob("/content/drive/MyDrive/datasetnew/*"):
    label = directory_path.split("/")[-1]
    for img_path in glob.glob(os.path.join(directory_path, "*")):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)
        img = cv2.resize(img, (SIZE, SIZE))
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        images.append(img)
        labels.append(label)

print(len(images))

#Convert lists to arrays
images = np.array(images)
labels = np.array(labels)
print(len(images))

#Encode labels from text to integers.
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(labels)
labels_encoded = le.transform(labels)

#Split data into test and train datasets (already split but assigning to meaningful convention)
x_data, y_data = images, labels_encoded

trainX, testX, trainY, testY = train_test_split(x_data, y_data, test_size=0.2, stratify=y_data)

import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.layers import (
    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense
)
import numpy as np


HAMNANet = models.Sequential()
#1st convolutional layer
HAMNANet.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding='same'))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
HAMNANet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))
#2nd convolutional layer
HAMNANet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
HAMNANet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#3rd Convolutional Layer
HAMNANet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))

#4th Convolutional Layer
HAMNANet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))

#5th Convolutional Layer
HAMNANet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
HAMNANet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#Passing it to a Fully Connected layer
HAMNANet.add(Flatten())
# 1st Fully Connected Layer
HAMNANet.add(Dense(4096, input_shape=(32,32,3,)))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
# Add Dropout to prevent overfitting
HAMNANet.add(Dropout(0.4))

#2nd Fully Connected Layer
HAMNANet.add(Dense(4096))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
#Add Dropout
HAMNANet.add(Dropout(0.4))

#3rd Fully Connected Layer
HAMNANet.add(Dense(1000))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('relu'))
#Add Dropout
HAMNANet.add(Dropout(0.4))

#Output Layer
HAMNANet.add(Dense(25))
HAMNANet.add(BatchNormalization())
HAMNANet.add(Activation('softmax'))


# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))
# model.add(layers.MaxPooling2D((2, 2)))

# model.add(layers.Conv2D(32, (3, 3), activation='relu'))
# model.add(layers.MaxPooling2D((2, 2)))

# model.add(layers.Flatten())
# model.add(layers.Dense(32, activation='relu'))
# model.add(layers.Dense(7, activation='softmax'))
HAMNANet.summary()

HAMNANet.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = HAMNANet.fit(trainX, trainY, epochs=100)

#model.save('/content/drive/MyDrive/modelhamna28nov12am')
HAMNANet.save('/content/drive/MyDrive/modelhamna28nov12am.keras')

#Saving the model in tf for android studio
tf.saved_model.save(HAMNANet, "/content/drive/MyDrive/tfmodel-android-facial-emotion-resize")

import tensorflow as tf
model = tf.keras.models.load_model('/content/drive/MyDrive/modelhamna28nov12am.keras')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_quantum_model = converter.convert() #Using Quantization Method to compress the tflite model

open('model.tflite', "wb").write(tflite_quantum_model)

score = model.evaluate(testX, testY, verbose=0)
print(score)

# plot graph epoch/loss during training
from matplotlib import pyplot

pyplot.gcf().set_size_inches(8,5)
pyplot.plot(history.history['accuracy'], label='accuracy')
pyplot.plot(history.history['loss'], linestyle='dotted', label='loss')
pyplot.legend(loc="right")
pyplot.xlabel("Epoch", fontsize=18)
pyplot.ylabel("Loss", fontsize=18)
ax = plt.gca()
ax.tick_params(axis='x', labelsize=14)
ax.tick_params(axis='y', labelsize=14)
pyplot.show()

SIZE = 256
#img = cv2.imread('/content/drive/MyDrive/mydata/desert/30.jpeg')
#img = cv2.imread('/content/drive/MyDrive/mydata/snow/3.jpeg')
img = cv2.imread('/content/drive/MyDrive/DATASETS/Cat Angry/02.jpg')
img = cv2.resize(img, (SIZE, SIZE))
img = np.expand_dims(img, axis=0)
print(model.predict(img))
print(le.inverse_transform([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]))